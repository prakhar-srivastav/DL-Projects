{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLpCjnh0k2Ag",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "5c2e20a7-eab1-4dc4-8a49-c13c90ac9c92"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH-WFHwP8JJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQMnSa4t8ZUg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "ac2271ad-eb56-4d4d-e06c-e25dbdd9214e"
      },
      "source": [
        "import os \n",
        "os.listdir('/content/drive/My Drive/Text generation')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['moby_dick_four_chapters.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqTJ-ko98fbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEaizPHh8593",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "38f66cf9-8439-428b-aaf9-97ff64c61639"
      },
      "source": [
        "tf.__version__\n",
        "\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e49FBgUi-7oK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path= '/content/drive/My Drive/Text generation/moby_dick_four_chapters.txt'\n",
        "text = open(path, 'r').read()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "975AR3ZC_h1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "b3107b56-fbe1-46cc-8cb3-f1b785fa4132"
      },
      "source": [
        "print(text[:500])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Call me Ishmael.  Some years ago--never mind how long\n",
            "precisely--having little or no money in my purse, and nothing\n",
            "particular to interest me on shore, I thought I would sail about a\n",
            "little and see the watery part of the world.  It is a way I have of\n",
            "driving off the spleen and regulating the circulation.  Whenever I\n",
            "find myself growing grim about the mouth; whenever it is a damp,\n",
            "drizzly November in my soul; whenever I find myself involuntarily\n",
            "pausing before coffin warehouses, and bringing up t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxPIumnX_jTH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "71aeb30c-1af9-4d24-b173-12a784674f9d"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(vocab)\n",
        "len(vocab)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '1', '2', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_q3N0Mz_nA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_to_ind = {u:i for i, u in enumerate(vocab)}"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO42M4ngAJy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6acb9793-e8ac-4aab-fcdb-5ec220c598e3"
      },
      "source": [
        "char_to_ind"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " \"'\": 4,\n",
              " '(': 5,\n",
              " ')': 6,\n",
              " ',': 7,\n",
              " '-': 8,\n",
              " '.': 9,\n",
              " '1': 10,\n",
              " '2': 11,\n",
              " ':': 12,\n",
              " ';': 13,\n",
              " '?': 14,\n",
              " 'A': 15,\n",
              " 'B': 16,\n",
              " 'C': 17,\n",
              " 'D': 18,\n",
              " 'E': 19,\n",
              " 'F': 20,\n",
              " 'G': 21,\n",
              " 'H': 22,\n",
              " 'I': 23,\n",
              " 'J': 24,\n",
              " 'K': 25,\n",
              " 'L': 26,\n",
              " 'M': 27,\n",
              " 'N': 28,\n",
              " 'O': 29,\n",
              " 'P': 30,\n",
              " 'Q': 31,\n",
              " 'R': 32,\n",
              " 'S': 33,\n",
              " 'T': 34,\n",
              " 'U': 35,\n",
              " 'V': 36,\n",
              " 'W': 37,\n",
              " 'Y': 38,\n",
              " 'Z': 39,\n",
              " 'a': 40,\n",
              " 'b': 41,\n",
              " 'c': 42,\n",
              " 'd': 43,\n",
              " 'e': 44,\n",
              " 'f': 45,\n",
              " 'g': 46,\n",
              " 'h': 47,\n",
              " 'i': 48,\n",
              " 'j': 49,\n",
              " 'k': 50,\n",
              " 'l': 51,\n",
              " 'm': 52,\n",
              " 'n': 53,\n",
              " 'o': 54,\n",
              " 'p': 55,\n",
              " 'q': 56,\n",
              " 'r': 57,\n",
              " 's': 58,\n",
              " 't': 59,\n",
              " 'u': 60,\n",
              " 'v': 61,\n",
              " 'w': 62,\n",
              " 'x': 63,\n",
              " 'y': 64,\n",
              " 'z': 65}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkiBbJpBALqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind_to_char = np.array(vocab)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNk_-w9DAOTN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "outputId": "e8c8131f-5b27-48cb-8a2d-5cd14234601d"
      },
      "source": [
        "ind_to_char"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '1', '2', ':',\n",
              "       ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n",
              "       'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y',\n",
              "       'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
              "       'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
              "       'z'], dtype='<U1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypMVmcKzAP2_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec0cd750-ee1c-4613-b722-cc90cc9e3d16"
      },
      "source": [
        "vocab"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '1',\n",
              " '2',\n",
              " ':',\n",
              " ';',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'Y',\n",
              " 'Z',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrODtgNvASq9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text = np.array([char_to_ind[c] for c in text])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRqIBTKoAY25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2df1b9c4-56ec-43cc-bf4f-24d6ef210bd1"
      },
      "source": [
        "encoded_text"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([17, 40, 51, ..., 54, 53,  9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8LS0lNeAad6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "93623053-189b-4e98-b8fc-cef712b3ea01"
      },
      "source": [
        "sample = text[:20]\n",
        "sample"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Call me Ishmael.  So'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6onOhIksA1vQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "8093ee72-7d49-4efa-9adf-f57c7fbfe657"
      },
      "source": [
        "encoded_text[:20]"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([17, 40, 51, 51,  1, 52, 44,  1, 23, 58, 47, 52, 40, 44, 51,  9,  1,\n",
              "        1, 33, 54])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M73cBR_wA4GN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "outputId": "3531c7a6-42f5-430f-d26d-7fadf911374d"
      },
      "source": [
        "print(text[:500])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Call me Ishmael.  Some years ago--never mind how long\n",
            "precisely--having little or no money in my purse, and nothing\n",
            "particular to interest me on shore, I thought I would sail about a\n",
            "little and see the watery part of the world.  It is a way I have of\n",
            "driving off the spleen and regulating the circulation.  Whenever I\n",
            "find myself growing grim about the mouth; whenever it is a damp,\n",
            "drizzly November in my soul; whenever I find myself involuntarily\n",
            "pausing before coffin warehouses, and bringing up t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yt1kv0BhBWlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len = 120"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUYWzFwBBjsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_num_seq = len(text)//(seq_len+1)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv-y1IpXBlcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "02534551-37f9-4f57-af02-b915be16d2fe"
      },
      "source": [
        "total_num_seq"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "509"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTejtD0fBml1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "\n",
        "# for i in char_dataset.take(500):\n",
        "#      print(ind_to_char[i.numpy()])"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL2dAdhQB8Mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjPKribbDBYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_seq_targets(seq):\n",
        "    input_txt = seq[:-1]\n",
        "    target_txt = seq[1:]\n",
        "    return input_txt, target_txt"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-hcR1ecDPlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = sequences.map(create_seq_targets)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ZrxvwiE8Px",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "5dfe8943-95a9-46fb-acc9-1ede1cd4461b"
      },
      "source": [
        "for input_txt, target_txt in  dataset.take(1):\n",
        "    print(input_txt.numpy())\n",
        "    print(''.join(ind_to_char[input_txt.numpy()]))\n",
        "    print('\\n')\n",
        "    print(target_txt.numpy())\n",
        "    # There is an extra whitespace!\n",
        "    print(''.join(ind_to_char[target_txt.numpy()]))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[17 40 51 51  1 52 44  1 23 58 47 52 40 44 51  9  1  1 33 54 52 44  1 64\n",
            " 44 40 57 58  1 40 46 54  8  8 53 44 61 44 57  1 52 48 53 43  1 47 54 62\n",
            "  1 51 54 53 46  0 55 57 44 42 48 58 44 51 64  8  8 47 40 61 48 53 46  1\n",
            " 51 48 59 59 51 44  1 54 57  1 53 54  1 52 54 53 44 64  1 48 53  1 52 64\n",
            "  1 55 60 57 58 44  7  1 40 53 43  1 53 54 59 47 48 53 46  0 55 40 57 59]\n",
            "Call me Ishmael.  Some years ago--never mind how long\n",
            "precisely--having little or no money in my purse, and nothing\n",
            "part\n",
            "\n",
            "\n",
            "[40 51 51  1 52 44  1 23 58 47 52 40 44 51  9  1  1 33 54 52 44  1 64 44\n",
            " 40 57 58  1 40 46 54  8  8 53 44 61 44 57  1 52 48 53 43  1 47 54 62  1\n",
            " 51 54 53 46  0 55 57 44 42 48 58 44 51 64  8  8 47 40 61 48 53 46  1 51\n",
            " 48 59 59 51 44  1 54 57  1 53 54  1 52 54 53 44 64  1 48 53  1 52 64  1\n",
            " 55 60 57 58 44  7  1 40 53 43  1 53 54 59 47 48 53 46  0 55 40 57 59 48]\n",
            "all me Ishmael.  Some years ago--never mind how long\n",
            "precisely--having little or no money in my purse, and nothing\n",
            "parti\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1V9q8gWE_e4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n",
        "# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\n",
        "buffer_size = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXagRXOHFG4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a801f7db-4d2a-47ab-cbb7-59b716f66efc"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X6rT8a-FH5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embed_dim = 64\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_neurons = 1026"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV1q3XiKFN2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCIZsmRXFi2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ig-wW-1QFkSY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "98d63f8c-ad1f-4fc7-9cce-9620fb9e1ae4"
      },
      "source": [
        "help(sparse_categorical_crossentropy)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on function sparse_categorical_crossentropy in module tensorflow.python.keras.losses:\n",
            "\n",
            "sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1)\n",
            "    Computes the sparse categorical crossentropy loss.\n",
            "    \n",
            "    Usage:\n",
            "    \n",
            "    >>> y_true = [1, 2]\n",
            "    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
            "    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
            "    >>> assert loss.shape == (2,)\n",
            "    >>> loss.numpy()\n",
            "    array([0.0513, 2.303], dtype=float32)\n",
            "    \n",
            "    Args:\n",
            "      y_true: Ground truth values.\n",
            "      y_pred: The predicted values.\n",
            "      from_logits: Whether `y_pred` is expected to be a logits tensor. By default,\n",
            "        we assume that `y_pred` encodes a probability distribution.\n",
            "      axis: (Optional) Defaults to -1. The dimension along which the entropy is\n",
            "        computed.\n",
            "    \n",
            "    Returns:\n",
            "      Sparse categorical crossentropy loss value.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxvmMF3CFlzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_cat_loss(y_true,y_pred):\n",
        "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh74ST75FoLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))\n",
        "    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))\n",
        "    # Final Dense Layer to Predict\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.compile(optimizer='adam', loss=sparse_cat_loss) \n",
        "    return model"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uRr8LVmFpaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embed_dim=embed_dim,\n",
        "  rnn_neurons=rnn_neurons,\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODpjUT7CF_cY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "72ddeea9-f95a-43e8-9462-91edb840ac4b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (128, None, 64)           4224      \n",
            "_________________________________________________________________\n",
            "gru_6 (GRU)                  (128, None, 1026)         3361176   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (128, None, 66)           67782     \n",
            "=================================================================\n",
            "Total params: 3,433,182\n",
            "Trainable params: 3,433,182\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJUKonC0GAXs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5dc8b542-1afc-4328-c431-b3a59087751a"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "\n",
        "  # Predict off some random batch\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "  # Display the dimensions of the predictions\n",
        "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 120, 66)  <=== (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvGIW7H5GBfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui93rg5oGDvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Sacan1iGFf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 300"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4bwOkibGH3S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "afb760a6-6534-4680-aa8f-1511723f0111"
      },
      "source": [
        "model.fit(dataset,epochs=epochs)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "3/3 [==============================] - 1s 168ms/step - loss: 4.1598\n",
            "Epoch 2/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 4.8808\n",
            "Epoch 3/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 3.9548\n",
            "Epoch 4/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 3.9417\n",
            "Epoch 5/300\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 3.7519\n",
            "Epoch 6/300\n",
            "3/3 [==============================] - 0s 146ms/step - loss: 3.6372\n",
            "Epoch 7/300\n",
            "3/3 [==============================] - 0s 144ms/step - loss: 3.5018\n",
            "Epoch 8/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 3.3386\n",
            "Epoch 9/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 3.1896\n",
            "Epoch 10/300\n",
            "3/3 [==============================] - 0s 144ms/step - loss: 3.0941\n",
            "Epoch 11/300\n",
            "3/3 [==============================] - 0s 145ms/step - loss: 3.1044\n",
            "Epoch 12/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 3.0814\n",
            "Epoch 13/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 3.0653\n",
            "Epoch 14/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 3.0491\n",
            "Epoch 15/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 3.0293\n",
            "Epoch 16/300\n",
            "3/3 [==============================] - 0s 146ms/step - loss: 3.0184\n",
            "Epoch 17/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 3.0088\n",
            "Epoch 18/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 2.9851\n",
            "Epoch 19/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.9661\n",
            "Epoch 20/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 2.9464\n",
            "Epoch 21/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.9270\n",
            "Epoch 22/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 2.8984\n",
            "Epoch 23/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 2.8655\n",
            "Epoch 24/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.8318\n",
            "Epoch 25/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 2.8005\n",
            "Epoch 26/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.7575\n",
            "Epoch 27/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.7292\n",
            "Epoch 28/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.6882\n",
            "Epoch 29/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 2.6566\n",
            "Epoch 30/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.6392\n",
            "Epoch 31/300\n",
            "3/3 [==============================] - 0s 146ms/step - loss: 2.6043\n",
            "Epoch 32/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.5869\n",
            "Epoch 33/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.5637\n",
            "Epoch 34/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.5397\n",
            "Epoch 35/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.5247\n",
            "Epoch 36/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.5064\n",
            "Epoch 37/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.4962\n",
            "Epoch 38/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 2.4825\n",
            "Epoch 39/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.4704\n",
            "Epoch 40/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 2.4550\n",
            "Epoch 41/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.4390\n",
            "Epoch 42/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.4272\n",
            "Epoch 43/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.4126\n",
            "Epoch 44/300\n",
            "3/3 [==============================] - 0s 146ms/step - loss: 2.4171\n",
            "Epoch 45/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.3904\n",
            "Epoch 46/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.3922\n",
            "Epoch 47/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 2.3778\n",
            "Epoch 48/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.3761\n",
            "Epoch 49/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.3655\n",
            "Epoch 50/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.3589\n",
            "Epoch 51/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.3509\n",
            "Epoch 52/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.3461\n",
            "Epoch 53/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 2.3431\n",
            "Epoch 54/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.3316\n",
            "Epoch 55/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 2.3175\n",
            "Epoch 56/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.3149\n",
            "Epoch 57/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 2.3183\n",
            "Epoch 58/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 2.3071\n",
            "Epoch 59/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.2980\n",
            "Epoch 60/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.2950\n",
            "Epoch 61/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.2883\n",
            "Epoch 62/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.2784\n",
            "Epoch 63/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.2760\n",
            "Epoch 64/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.2751\n",
            "Epoch 65/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 2.2586\n",
            "Epoch 66/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 2.2596\n",
            "Epoch 67/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.2487\n",
            "Epoch 68/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.2411\n",
            "Epoch 69/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.2353\n",
            "Epoch 70/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.2307\n",
            "Epoch 71/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.2270\n",
            "Epoch 72/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.2127\n",
            "Epoch 73/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 2.2100\n",
            "Epoch 74/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.1975\n",
            "Epoch 75/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.1957\n",
            "Epoch 76/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.1937\n",
            "Epoch 77/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 2.1781\n",
            "Epoch 78/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.1771\n",
            "Epoch 79/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.1701\n",
            "Epoch 80/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.1607\n",
            "Epoch 81/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 2.1503\n",
            "Epoch 82/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.1424\n",
            "Epoch 83/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.1383\n",
            "Epoch 84/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 2.1288\n",
            "Epoch 85/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.1160\n",
            "Epoch 86/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 2.1187\n",
            "Epoch 87/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.1004\n",
            "Epoch 88/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 2.0967\n",
            "Epoch 89/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 2.0920\n",
            "Epoch 90/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.0880\n",
            "Epoch 91/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 2.0705\n",
            "Epoch 92/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 2.0659\n",
            "Epoch 93/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.0530\n",
            "Epoch 94/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 2.0550\n",
            "Epoch 95/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.0367\n",
            "Epoch 96/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 2.0181\n",
            "Epoch 97/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.0251\n",
            "Epoch 98/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 2.0095\n",
            "Epoch 99/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 2.0000\n",
            "Epoch 100/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 1.9936\n",
            "Epoch 101/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.9883\n",
            "Epoch 102/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 1.9808\n",
            "Epoch 103/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.9768\n",
            "Epoch 104/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.9578\n",
            "Epoch 105/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 1.9523\n",
            "Epoch 106/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.9470\n",
            "Epoch 107/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 1.9312\n",
            "Epoch 108/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.9209\n",
            "Epoch 109/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 1.9179\n",
            "Epoch 110/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.9066\n",
            "Epoch 111/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.9012\n",
            "Epoch 112/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 1.8899\n",
            "Epoch 113/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 1.8939\n",
            "Epoch 114/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 1.8793\n",
            "Epoch 115/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.8692\n",
            "Epoch 116/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.8506\n",
            "Epoch 117/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.8497\n",
            "Epoch 118/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.8330\n",
            "Epoch 119/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.8175\n",
            "Epoch 120/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 1.8075\n",
            "Epoch 121/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 1.8066\n",
            "Epoch 122/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.8024\n",
            "Epoch 123/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 1.7823\n",
            "Epoch 124/300\n",
            "3/3 [==============================] - 0s 157ms/step - loss: 1.7768\n",
            "Epoch 125/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 1.7607\n",
            "Epoch 126/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.7528\n",
            "Epoch 127/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.7509\n",
            "Epoch 128/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 1.7342\n",
            "Epoch 129/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 1.7114\n",
            "Epoch 130/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 1.7110\n",
            "Epoch 131/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 1.6972\n",
            "Epoch 132/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 1.6846\n",
            "Epoch 133/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.6776\n",
            "Epoch 134/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 1.6687\n",
            "Epoch 135/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.6546\n",
            "Epoch 136/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.6408\n",
            "Epoch 137/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 1.6249\n",
            "Epoch 138/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.6085\n",
            "Epoch 139/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.6012\n",
            "Epoch 140/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.5877\n",
            "Epoch 141/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.5676\n",
            "Epoch 142/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 1.5714\n",
            "Epoch 143/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 1.5509\n",
            "Epoch 144/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 1.5389\n",
            "Epoch 145/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 1.5150\n",
            "Epoch 146/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 1.5040\n",
            "Epoch 147/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.4859\n",
            "Epoch 148/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.4753\n",
            "Epoch 149/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.4636\n",
            "Epoch 150/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 1.4442\n",
            "Epoch 151/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 1.4262\n",
            "Epoch 152/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.4026\n",
            "Epoch 153/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.3959\n",
            "Epoch 154/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.3946\n",
            "Epoch 155/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.3798\n",
            "Epoch 156/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 1.3611\n",
            "Epoch 157/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.3426\n",
            "Epoch 158/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 1.3223\n",
            "Epoch 159/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.2945\n",
            "Epoch 160/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.2796\n",
            "Epoch 161/300\n",
            "3/3 [==============================] - 0s 157ms/step - loss: 1.2668\n",
            "Epoch 162/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 1.2356\n",
            "Epoch 163/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 1.2183\n",
            "Epoch 164/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.2039\n",
            "Epoch 165/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.1858\n",
            "Epoch 166/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 1.1732\n",
            "Epoch 167/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 1.1476\n",
            "Epoch 168/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 1.1215\n",
            "Epoch 169/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 1.0997\n",
            "Epoch 170/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.0845\n",
            "Epoch 171/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.0603\n",
            "Epoch 172/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 1.0429\n",
            "Epoch 173/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 1.0250\n",
            "Epoch 174/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 1.0027\n",
            "Epoch 175/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.9984\n",
            "Epoch 176/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 0.9652\n",
            "Epoch 177/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.9519\n",
            "Epoch 178/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.9221\n",
            "Epoch 179/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.9093\n",
            "Epoch 180/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.8813\n",
            "Epoch 181/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.8582\n",
            "Epoch 182/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.8404\n",
            "Epoch 183/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.8070\n",
            "Epoch 184/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 0.7991\n",
            "Epoch 185/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.7795\n",
            "Epoch 186/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.7490\n",
            "Epoch 187/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.7234\n",
            "Epoch 188/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.7082\n",
            "Epoch 189/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 0.6905\n",
            "Epoch 190/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.6720\n",
            "Epoch 191/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.6488\n",
            "Epoch 192/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.6287\n",
            "Epoch 193/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 0.6106\n",
            "Epoch 194/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.5916\n",
            "Epoch 195/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.5666\n",
            "Epoch 196/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.5522\n",
            "Epoch 197/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.5356\n",
            "Epoch 198/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.5181\n",
            "Epoch 199/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 0.5116\n",
            "Epoch 200/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.4892\n",
            "Epoch 201/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.4812\n",
            "Epoch 202/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.4589\n",
            "Epoch 203/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.4471\n",
            "Epoch 204/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.4340\n",
            "Epoch 205/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.4261\n",
            "Epoch 206/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.4158\n",
            "Epoch 207/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.3997\n",
            "Epoch 208/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.3919\n",
            "Epoch 209/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.3784\n",
            "Epoch 210/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.3715\n",
            "Epoch 211/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.3649\n",
            "Epoch 212/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.3521\n",
            "Epoch 213/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.3418\n",
            "Epoch 214/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.3413\n",
            "Epoch 215/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.3260\n",
            "Epoch 216/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.3227\n",
            "Epoch 217/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.3125\n",
            "Epoch 218/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.3130\n",
            "Epoch 219/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.3038\n",
            "Epoch 220/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 0.2970\n",
            "Epoch 221/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.2889\n",
            "Epoch 222/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.2882\n",
            "Epoch 223/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.2787\n",
            "Epoch 224/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.2773\n",
            "Epoch 225/300\n",
            "3/3 [==============================] - 0s 156ms/step - loss: 0.2704\n",
            "Epoch 226/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.2626\n",
            "Epoch 227/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.2628\n",
            "Epoch 228/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.2609\n",
            "Epoch 229/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 0.2583\n",
            "Epoch 230/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.2517\n",
            "Epoch 231/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.2513\n",
            "Epoch 232/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.2475\n",
            "Epoch 233/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.2408\n",
            "Epoch 234/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.2378\n",
            "Epoch 235/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.2407\n",
            "Epoch 236/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.2374\n",
            "Epoch 237/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.2358\n",
            "Epoch 238/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.2353\n",
            "Epoch 239/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.2348\n",
            "Epoch 240/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.2263\n",
            "Epoch 241/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.2252\n",
            "Epoch 242/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.2222\n",
            "Epoch 243/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.2246\n",
            "Epoch 244/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.2180\n",
            "Epoch 245/300\n",
            "3/3 [==============================] - 0s 156ms/step - loss: 0.2180\n",
            "Epoch 246/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.2175\n",
            "Epoch 247/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.2191\n",
            "Epoch 248/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.2115\n",
            "Epoch 249/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 0.2107\n",
            "Epoch 250/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.2086\n",
            "Epoch 251/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.2094\n",
            "Epoch 252/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 0.2018\n",
            "Epoch 253/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.2036\n",
            "Epoch 254/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.2006\n",
            "Epoch 255/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.2042\n",
            "Epoch 256/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1965\n",
            "Epoch 257/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.1949\n",
            "Epoch 258/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1993\n",
            "Epoch 259/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.1979\n",
            "Epoch 260/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1947\n",
            "Epoch 261/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.1953\n",
            "Epoch 262/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.1884\n",
            "Epoch 263/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.1916\n",
            "Epoch 264/300\n",
            "3/3 [==============================] - 0s 148ms/step - loss: 0.1885\n",
            "Epoch 265/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.1896\n",
            "Epoch 266/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1880\n",
            "Epoch 267/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1877\n",
            "Epoch 268/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1826\n",
            "Epoch 269/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1805\n",
            "Epoch 270/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1831\n",
            "Epoch 271/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1785\n",
            "Epoch 272/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.1841\n",
            "Epoch 273/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.1755\n",
            "Epoch 274/300\n",
            "3/3 [==============================] - 0s 154ms/step - loss: 0.1779\n",
            "Epoch 275/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.1759\n",
            "Epoch 276/300\n",
            "3/3 [==============================] - 0s 153ms/step - loss: 0.1783\n",
            "Epoch 277/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1792\n",
            "Epoch 278/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.1780\n",
            "Epoch 279/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1767\n",
            "Epoch 280/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1738\n",
            "Epoch 281/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.1727\n",
            "Epoch 282/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1725\n",
            "Epoch 283/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.1699\n",
            "Epoch 284/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1685\n",
            "Epoch 285/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.1707\n",
            "Epoch 286/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1680\n",
            "Epoch 287/300\n",
            "3/3 [==============================] - 0s 147ms/step - loss: 0.1702\n",
            "Epoch 288/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1671\n",
            "Epoch 289/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1674\n",
            "Epoch 290/300\n",
            "3/3 [==============================] - 0s 156ms/step - loss: 0.1686\n",
            "Epoch 291/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.1636\n",
            "Epoch 292/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.1635\n",
            "Epoch 293/300\n",
            "3/3 [==============================] - 0s 155ms/step - loss: 0.1615\n",
            "Epoch 294/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.1613\n",
            "Epoch 295/300\n",
            "3/3 [==============================] - 0s 152ms/step - loss: 0.1635\n",
            "Epoch 296/300\n",
            "3/3 [==============================] - 0s 150ms/step - loss: 0.1585\n",
            "Epoch 297/300\n",
            "3/3 [==============================] - 0s 156ms/step - loss: 0.1626\n",
            "Epoch 298/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1592\n",
            "Epoch 299/300\n",
            "3/3 [==============================] - 0s 149ms/step - loss: 0.1608\n",
            "Epoch 300/300\n",
            "3/3 [==============================] - 0s 151ms/step - loss: 0.1590\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f81de3478d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z23Rcd8HGI_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('moby.h5') "
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7A3bCJdsKfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhQuvGAFtFwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
        "\n",
        "model.load_weights('moby.h5')\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_1S8m-ItHXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "6d375b93-19e1-42b4-e63d-296309b53eb6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (1, None, 64)             4224      \n",
            "_________________________________________________________________\n",
            "gru_7 (GRU)                  (1, None, 1026)           3361176   \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (1, None, 66)             67782     \n",
            "=================================================================\n",
            "Total params: 3,433,182\n",
            "Trainable params: 3,433,182\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aPD95yJtNAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
        "  '''\n",
        "  model: Trained Model to Generate Text\n",
        "  start_seed: Intial Seed text in string form\n",
        "  gen_size: Number of characters to generate\n",
        "\n",
        "  Basic idea behind this function is to take in some seed text, format it so\n",
        "  that it is in the correct shape for our network, then loop the sequence as\n",
        "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
        "  time series problems.\n",
        "  '''\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = gen_size\n",
        "\n",
        "  # Vecotrizing starting seed text\n",
        "  input_eval = [char_to_ind[s] for s in start_seed]\n",
        "\n",
        "  # Expand to match batch format shape\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty list to hold resulting generated text\n",
        "  text_generated = []\n",
        "\n",
        "  # Temperature effects randomness in our resulting text\n",
        "  # The term is derived from entropy/thermodynamics.\n",
        "  # The temperature is used to effect probability of next characters.\n",
        "  # Higher probability == lesss surprising/ more expected\n",
        "  # Lower temperature == more surprising / less expected\n",
        " \n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "\n",
        "  for i in range(num_generate):\n",
        "\n",
        "      # Generate Predictions\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      # Remove the batch shape dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # Use a cateogircal disitribution to select the next character\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # Pass the predicted charracter for the next input\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      # Transform back to character letter\n",
        "      text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed + ''.join(text_generated))"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBvo7og4tOpD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "a92f545f-d332-4aab-b0c3-63f53efbe548"
      },
      "source": [
        "\n",
        "print(generate_text(model,\"The\",gen_size=1000))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Offite may in.\n",
            "\n",
            "Brad chose\n",
            "world's with the hair\n",
            "on.  Place meselvide of the leggom in the sounds of the tinkly pleas.  But these warlay canclight\n",
            "to be hight; the counterpane, I arm thrown ovey my blooken's sates--night semething his a meanard\n",
            "ly.  I concluded that this harpooneer, in the course very lare, and he id to curere syent, but stop; could 't all remembering what the\n",
            "landlord said about the Gar semmer wes with\n",
            "leggo pactious condlinded jorty tceay reador you could and came\n",
            "near boor was new\n",
            "betan from thens,\" but I began to feel ous bettlep, I fectullous do tow in the air which smourd\n",
            "in the wird\n",
            "clene to a dime jistincuin begous.  Besides, I ain't insured.\"\n",
            "\n",
            "This being told to Queequeg, he at once complied, and again the strangest\n",
            "possible manners.  His education was not yet completed.  He was an\n",
            "upper oun on the coanser deef quiets and jolly there.  Fere was no tomowh, frim that I am in the habit of going to see hours you and sheper drowned.  But that some of monning them \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pY0pZORtQJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}